# Implementation of different policies

## Intro

### Import

import numpy as np
import random
import pandas as pd

### Objet : Markov Decision Process

class MarkovDecisionProcess:
    
    def __init__(self,N,S,A,Lambda,alpha,gamma,mu):
        #self.nr=0
        #self.np=0
        #self.nt=0
        self.N=N #The number of instances available
        self.S=S #The different states
        self.A=A #The different actions stored in an array (A[s] will be the set of actions of the action s)
        self.R=dict() #The rewards
        self.P=dict() #The different probabilities
        self.T=dict() #The different transition rates
        
        #The transition rates:
        self.Lambda=Lambda #the transition rate to a new job
        self.alpha=alpha #the transition rate for initialization
        self.gamma=gamma #the transition rate for a server to shut down
        self.mu=mu #the transition rate for a warm server to complete the request #=10ms
        self.U=(alpha+gamma+mu)*N+Lambda
        
      
    def Reward(self,s):
        if s not in self.R : #and self.nr <= 1000000
            self.R[s]=Reward_aux(s)
            #self.nr+=1
        return self.R[s]

    
    def Trans(self,s_next,s,a):
        if (s_next,s,a) not in self.T : #and self.nt <= 1000000
            diff=np.array(s_next)-np.array(s)
            if np.array_equal(diff,[0,0,-1,0]): #a server goes down
                self.T[s_next,s,a]=(gamma*s[2]) # gamma*w_0 
            elif np.array_equal(diff,[0,0,1,-1]): #a busy-warm server has made a request and becomes warm
                self.T[s_next,s,a]=(mu*s[3]) # mu*w_1
            elif np.array_equal(diff,[-1,0,1,0]): #a server i_0 becomes warm (end of init time)
                self.T[s_next,s,a]=(alpha*s[0]) # alpha*i_0
            elif np.array_equal(diff,[0,-1,0,1]): #an i_1 server becomes warm-busy (end of init time)
                self.T[s_next,s,a]=(alpha*s[1]) # alpha*i_1
            elif np.array_equal(diff,[a,0,-1,1]): # and w_0>0 and the action is feasible #a new request arrives, we turn on a server in i_0 and the new request is realized by a warm server (previously w_0) that we put in w_1
                self.T[s_next,s,a]=(Lambda)*int(s[0]+s[1]+s[2]+s[3]+a <= N)*int(s[2]>0) # Lambda
            elif np.array_equal(diff,[a-1,1,0,0]): # and w_0=0 and (i_0>0 or i_0= but a>0) and the action is feasible #a new request arrives, we turn on a server in i_0 and the new request is carried out by a server in i_0 that we put in i_1
                self.T[s_next,s,a]=(Lambda)*int(s[0]+s[1]+s[2]+s[3]+a <= N)*int(s[2]==0)*int(s[0]>0 or (s[0]==0 and a>0)) # Lambda
            elif np.array_equal(diff,[0,0,0,0]):
                self.T[s_next,s,a]=(-(gamma*s[2]+mu*s[3]+alpha*s[0]+alpha*s[1]+Lambda*int(s[0]+s[1]+s[2]+s[3]+a <= N)*int(s[0]>0 or s[2]>0 or a>0)))
            else:
                self.T[s_next,s,a]=0
        return self.T[s_next,s,a]

    
    def Proba(self,s_next,s,a):
        if (s_next,s,a) not in self.P : #and self.np <= 1000000
            if np.array_equal(s_next,s):
                self.P[s_next,s,a]=1-(gamma*s[2]+mu*s[3]+alpha*s[0]+alpha*s[1]+Lambda*int(s[0]+s[1]+s[2]+s[3]+a<=N)*int(s[0]>0 or s[2]>0 or a>0))/(M.U)
            else:
                self.P[s_next,s,a]=M.Trans(s_next,s,a)/(M.U)
        return self.P[s_next,s,a]

### Creation of the different functions actions and rewards according to the states

C_1 = 1 #the cost of processing a request
#C_2 = 10 #the penalty for keeping the customer waiting


def Reward_aux(s): #s=(i_0,i_1,w_0,w_1)
    return(-((s[0]+s[1]+s[2]+s[3])*C_1+(s[1]*C_2)))

def actions(s): #the actions i.e. the delta value that can be chosen
    actions=list()
    if s[0]==0 and s[2]==0 and sum(s)<N:
        for i in range(1,N+1-s[0]-s[1]-s[2]-s[3]):
            actions.append(i)
    else:
        for i in range(N+1-s[0]-s[1]-s[2]-s[3]):
            actions.append(i)
    return(actions)

### Variables N (number of available servers), S (set of states) and A (set of actions)

N=10

#States (we create them globally because they will not change afterwards)

S=list()
for h in range(N+1):
    for i in range(N-h+1):
        for j in range(N-h-i+1):
            for k in range(N-h-i-j+1):
                S.append((h,i,j,k))

#Actions (idem)

A=[]
for h in range(N+1):
    for i in range(N-h+1):
        for j in range(N-h-i+1):
            for k in range(N-h-i-j+1):
                A.append(actions([h,i,j,k]))

### Creation of the different variables

Lambda=N/2 #the transition rate to a new job
alpha=0.1 #the transition rate for initialization
gamma=0.01 #the transition rate for a server to shut down
mu=1 #the transition rate for a warm server to complete the request #=10ms

M = MarkovDecisionProcess(N,S,A,Lambda,alpha,gamma,mu)

max_iter = 1000  # Maximum number of iterations
discount = 0.99

## Programme Value iteration

### Creation of the identity matrix

I=np.eye(len(M.S))

### Start of the Value Iteration program

V = np.zeros(len(S))  # Initialize values
V_new = np.zeros(len(S))
pi = np.zeros(len(S))

# The constants for the rewards

C_1 = 1 #the cost of processing a request

df=pd.DataFrame(S)
mapping = {df.columns[0]: 'i_0', df.columns[1]: 'i_1', df.columns[2]:'w_0', df.columns[3]:'w_1'} 
su = df.rename(columns=mapping) 

H=[]
K=[]

delta = 1e-1  # Error tolerance


for C_2 in range(1,11):
    M=MarkovDecisionProcess(N,S,A,Lambda,alpha,gamma,mu)
    
    #Creation of the reward vector
    
    R=[]
    for s in M.S:
        R.append(M.Reward(s))
    
    max_diff = 10000 # For the while begin
    
#################################################    
    
    
    compt = 0 # Iteration counter

    # Start value iteration
    while compt < max_iter and max_diff > delta :

        #update compteur
        compt+=1

        # Initialize max difference
        max_diff = 0 
        
        P=np.zeros((len(M.S),len(M.S)))
        for i in range(len(M.S)):
            for a in actions(S[i]):
                for j in range(len(M.S)):
                    P[i][j]=M.Proba(S[j],S[i],a)
                    
                val=R+discount*(P@V)
                if a == actions(S[i])[0]:
                    V_new[i] = val[i]
                    best_a = a                    
                else:
                    if V_new[i] < val[i]:
                        V_new[i] = val[i]
                        best_a = a
                        
            pi[i] = best_a #update best policy

        # Update maximum difference
        max_diff = max(abs(V - V_new))


        print(max_diff)
        print(compt)
            
        # Update value functions    
        V = V_new.copy()
        #print(s_0,pi[S.index(s_0)])

        #s_0=Ã©tat_next(s_0,pi[S.index(s_0)])
    Serie=pd.Series(np.array(pi))
    su=pd.concat([su,Serie.rename(C_2)],axis=1)
    
    for s in M.S:
        print(C_2,s,pi[M.S.index(s)])
        
        
#################################

    H.append(V.copy())
    if len(H)>1:
        K.append(np.array(H[-1])-np.array(H[-2]))
    else:
        K.append(np.zeros(len(V)))
    
    V+=K[-1]
    
#################################


su.to_excel(f'VI_N{N}_L{Lambda}_V2.xlsx')

